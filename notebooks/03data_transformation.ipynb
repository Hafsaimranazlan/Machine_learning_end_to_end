{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Machine_learning_end_to_end\\\\machine_learning\\\\notebooks'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f:\\\\Machine_learning_end_to_end\\\\machine_learning'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataTransformationConfig:\n",
    "    root_dir:Path\n",
    "    data_path:Path\n",
    "    pkl_path:Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.constant import *\n",
    "from src.utils.common import read_yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.components import data_transformation\n",
    "\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(self,\n",
    "                        config_path=CONFIG_FILE_PATH,\n",
    "                        schema_path=SCHEMA_FILE_PATH,\n",
    "                        params_path=PARAMS_FILE_PATH):\n",
    "        self.config=read_yaml(config_path)\n",
    "        self.schema=read_yaml(schema_path)\n",
    "        self.params=read_yaml(params_path)\n",
    "\n",
    "    def data_transformation_config(self)->DataTransformationConfig:\n",
    "        config=self.config.data_transformation\n",
    "        data_transformation_config=DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "            pkl_path=config.pkl_path\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from src import logger\n",
    "from pathlib import Path\n",
    "from src.utils.common import save_bin\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,config:DataTransformationConfig):\n",
    "        self.config=config\n",
    "\n",
    "    def data_transformation(self,data):\n",
    "        logger.info(\"data transformation object start creating \\n\")\n",
    "        column=data.columns\n",
    "        column_save=column.drop(\"quality\")\n",
    "\n",
    "        p1=Pipeline(steps=[\n",
    "        ('standard_scaler',StandardScaler())\n",
    "        ])\n",
    "\n",
    "        ct=ColumnTransformer(transformers=[\n",
    "        ('std_scaler',p1,column_save)\n",
    "        ])\n",
    "        logger.info(\"data Transformation object creation completed \")\n",
    "        return ct\n",
    "    \n",
    "\n",
    "    def train_test_split(self):\n",
    "        data=pd.read_csv(self.config.data_path)\n",
    "\n",
    "        logger.info(\"data are splitted into train and test\")\n",
    "\n",
    "        train_data,test_data=train_test_split(data,test_size=0.2,random_state=5)\n",
    "        logger.info(f\"train data shape {train_data.shape}\")\n",
    "        logger.info(f\"test data shape {test_data.shape}\")\n",
    "\n",
    "        logger.info(f\"train data column : {list(train_data.columns)}\\n\")\n",
    "        logger.info(f\"test data column {list(test_data.columns)}\\n\")\n",
    "\n",
    "        logger.info(\"train,test data saved to path\")\n",
    "        train_data.to_csv(os.path.join(self.config.root_dir,\"train.csv\"),index=False)\n",
    "        test_data.to_csv(os.path.join(self.config.root_dir,\"test.csv\"),index=False)\n",
    "\n",
    "        logger.info(\"preprocessor object\")\n",
    "        preproc_obj=self.data_transformation(train_data)\n",
    "\n",
    "        TARGET_COLUMN='quality'\n",
    "        logger.info(\"splitting target column from train data\")\n",
    "        train_df=train_data.drop(TARGET_COLUMN,axis=1)\n",
    "        target_train=train_data[TARGET_COLUMN]\n",
    "        logger.info(\"Target column splitted from train data\")\n",
    "\n",
    "        logger.info(\"splitting target column from test data\")\n",
    "        test_df=test_data.drop(TARGET_COLUMN,axis=1)\n",
    "        target_test=test_data[TARGET_COLUMN]\n",
    "        logger.info(\"target column splitted from test data\")\n",
    "\n",
    "        logger.info(\"Transform data\")\n",
    "        train_trans=preproc_obj.fit_transform(train_df)\n",
    "        test_trans=preproc_obj.transform(test_df)\n",
    "        logger.info(\"save preprocessor object\")\n",
    "        path=Path(self.config.pkl_path)\n",
    "        save_bin(preproc_obj,path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-13 20:23:39,519 : INFO : common : yaml file path : config\\config.yaml loaded successfully:]\n",
      "[2024-11-13 20:23:39,543 : INFO : common : yaml file path : schema.yaml loaded successfully:]\n",
      "[2024-11-13 20:23:39,551 : INFO : common : yaml file path : params.yaml loaded successfully:]\n",
      "[2024-11-13 20:23:40,012 : INFO : 958572087 : data are splitted into train and test]\n",
      "[2024-11-13 20:23:40,147 : INFO : 958572087 : train data shape (914, 12)]\n",
      "[2024-11-13 20:23:40,150 : INFO : 958572087 : test data shape (229, 12)]\n",
      "[2024-11-13 20:23:40,153 : INFO : 958572087 : train data column : ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "]\n",
      "[2024-11-13 20:23:40,156 : INFO : 958572087 : test data column ['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar', 'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density', 'pH', 'sulphates', 'alcohol', 'quality']\n",
      "]\n",
      "[2024-11-13 20:23:40,161 : INFO : 958572087 : train,test data saved to path]\n",
      "[2024-11-13 20:23:40,241 : INFO : 958572087 : preprocessor object]\n",
      "[2024-11-13 20:23:40,247 : INFO : 958572087 : data transformation object start creating \n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-13 20:23:40,794 : INFO : 958572087 : data Transformation object creation completed ]\n",
      "[2024-11-13 20:23:40,797 : INFO : 958572087 : splitting target column from train data]\n",
      "[2024-11-13 20:23:41,129 : INFO : 958572087 : Target column splitted from train data]\n",
      "[2024-11-13 20:23:41,134 : INFO : 958572087 : splitting target column from test data]\n",
      "[2024-11-13 20:23:41,140 : INFO : 958572087 : target column splitted from test data]\n",
      "[2024-11-13 20:23:41,145 : INFO : 958572087 : Transform data]\n",
      "[2024-11-13 20:23:41,753 : INFO : 958572087 : save preprocessor object]\n",
      "[2024-11-13 20:23:42,005 : INFO : common : binary file save to : data\\processed\\prepro.pkl]\n"
     ]
    }
   ],
   "source": [
    "obj=ConfigurationManager()\n",
    "data_transform=obj.data_transformation_config()\n",
    "\n",
    "obj2=DataTransformation(data_transform)\n",
    "obj2.train_test_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
